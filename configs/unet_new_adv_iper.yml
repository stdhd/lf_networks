experiment_id: sep_motion_batchnorm_start_encoder
experiment: firststagegan
model_save_dir: data/models
num_workers: 24
max_frames: 5
split: 0.5
split_direction: samples
watch_gradients: False
logging:
  video: True
  n_fvd_samples: 1000
  n_samples: 20
  bs_i3d: 8

loss_weights:
  l1: 1.
  vgg: 1.
  gan: 1.
  q_latent: 0.
  p_codebook: 0.
  disc_iter_start: 30000
  enable_adversarial: True

latent_dimensions:
  pq_latent_size: 32
  pq_latent_dim: 8 # 64 x 3 x 3
  # Double the size is the latent output of hamiltonian unet
# Define networks architectures
networks:
  variational: True
  dtype : "float"
  start_encoder:
    type: "standard"
    hidden_conv_layers: 2
    n_filters: [ 8, 12, 32 ]  # first + hidden
    kernel_sizes: [ 3, 3, 3, 3, 3]  # first + hidden + last
    strides: [ 1, 2, 2, 2, 2]  # first + hidden + last
    out_channels: 32 # TODO: Evaluate impact

  motion_encoder:
    blocks_per_layer: 2
    ENC_M_channels: [ 8, 16, 32, 64, 128 ] #[ 4, 8, 8, 16, 32 ]
    strides: [ 1, 2, 1, 2, 2 ]
    z_dim: 2048
    img_size: 64
    do_reparameterization: False
    num_groups: [8, 8, 8, 8, 8]

  unet:
    out_channels: 64 #32
    bilinear: False
    use_attention: False
    final_fc: False
    use_self_attention: False
    use_cross_attention: False
    #intermediate_fc: True
    #input_size: 32
    #hidden_sizes: 32
    #kernel_sizes: 1
    #n_layers: 2

  decoder:
    n_residual_blocks: 3
    n_filters: [32, 16, 8]
    kernel_sizes: [3, 3, 3, 3]

# Define HGN Integrator torch.Size([2, 16, 9, 9]) torch.Size([2, 3, 72, 72]) torch.Size([1, 48, 72, 72]) torch.Size([1, 3, 72, 72])
integrator:
  method: "Leapfrog"
  learnable_delta_t: False

# Define optimization
optimization:
  epochs: 100
  precision: 32
  batch_size: 32
  input_frames: 10  # Number of frames to feed to the encoder while training
  # Learning rates
  learning_rate: 1.5e-4
  hnn_lr: 1.5e-4
  decoder_lr: 1.5e-4
  frame_encoder_lr: 1.5e-4
  motion_encoder_lr: 1.5e-4
  codebook_lr: 1.5e-4
  disc_t_lr: 1.5e-4
  integrator_lr: 1.5e-4
  training_objective: 'prior' # { prior, posterior }
  gamma: 0.9 # scheduler
  spade_frame_equals_previous: False
  weight_decay: 0.0001
  scheduled_sampling: False
  detach_previous_frame_and_q: False
  use_integrated_q_instead_of_encoded: True
  prob_use_true_q: 0

# Define data characteristics
dataset:
  datakeys: ['images']
  name: 'TaichiDataset'
  sequence_length: 10
  dataset: TaichiDataset
  poke_size: 1
#  valid_lags: 1
  #subsample_step: 2
  max_frames: 10
  batch_size: 25
  n_workers: 20
  yield_videos: True
  spatial_size: !!python/tuple [64,64]
  p_col: .8
  p_geom: .8
  augment_b: 0.4
  augment_c: 0.5
  augment_h: 0.15
  augment_s: 0.4
  aug_deg: 15 # for iper use 0, for plants use 30Â°
  # translation is (vertical, horizontal)
  aug_trans: !!python/tuple [0.1,0.1]
  split: official
  flow_weights: False
  filter_flow: False
  augment_wo_dis: True
  n_pokes: 0
  normalize_flows: False
  zero_poke: True
  zero_poke_amount: 0
  object_weighting: False
  val_obj_weighting: False
  rollout:
    delta_time: 1
    n_channels: 3
    noise_level: 1  # Level of environment noise. 0 means no noise, 1 means max noise.
                    # Maximum values are defined in each environment.

